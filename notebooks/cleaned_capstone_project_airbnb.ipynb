{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: data was imported from http://insideairbnb.com/get-the-data.html\n",
    "\n",
    "London data was pulled on 11 April 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening the reviews csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries here\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, ElasticNet, ElasticNetCV, Lasso, LassoCV, RidgeCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import confusion_matrix, classification_report, plot_precision_recall_curve, plot_roc_curve\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import scikitplot as skplt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import geopy.distance\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/listings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74840, 74)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary - Listings\n",
    "\n",
    "Below are the steps I took to help create the data dictionary csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null_values = []\n",
    "\n",
    "# for column in df.columns:\n",
    "#     null_values.append(df[column].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df = pd.DataFrame({'Field': df.columns,'null_values': null_values})\n",
    "# data_df.set_index('Field',drop=True,inplace=True)\n",
    "# data_df['type'] = df.dtypes\n",
    "# data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df.to_csv(\"../notebooks/data_dict.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning - Listings\n",
    "\n",
    "For the first model, we're going to try to predict the price of an Airbnb property using the listings features. I'm going to alter the dataset, to provide a foundation dataset that I'll be able to use for all of my models. I'll then be able to customise what kind of tweaks I do to the foundation data depending on which model I'm fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'listing_url',\n",
       " 'scrape_id',\n",
       " 'last_scraped',\n",
       " 'name',\n",
       " 'description',\n",
       " 'neighborhood_overview',\n",
       " 'picture_url',\n",
       " 'host_id',\n",
       " 'host_since',\n",
       " 'host_about',\n",
       " 'host_response_time',\n",
       " 'host_response_rate',\n",
       " 'host_acceptance_rate',\n",
       " 'host_is_superhost',\n",
       " 'host_verifications',\n",
       " 'host_has_profile_pic',\n",
       " 'host_identity_verified',\n",
       " 'neighbourhood',\n",
       " 'neighbourhood_cleansed',\n",
       " 'neighbourhood_group_cleansed',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bathrooms_text',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'amenities',\n",
       " 'price',\n",
       " 'minimum_nights',\n",
       " 'maximum_nights',\n",
       " 'minimum_minimum_nights',\n",
       " 'maximum_minimum_nights',\n",
       " 'minimum_maximum_nights',\n",
       " 'maximum_maximum_nights',\n",
       " 'minimum_nights_avg_ntm',\n",
       " 'maximum_nights_avg_ntm',\n",
       " 'calendar_updated',\n",
       " 'has_availability',\n",
       " 'availability_30',\n",
       " 'availability_60',\n",
       " 'availability_90',\n",
       " 'availability_365',\n",
       " 'calendar_last_scraped',\n",
       " 'license',\n",
       " 'instant_bookable',\n",
       " 'calculated_host_listings_count']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns that are based on review data\n",
    "\n",
    "review_cols_drop = ['number_of_reviews', 'number_of_reviews_ltm', 'number_of_reviews_l30d',\n",
    " 'first_review', 'last_review', 'review_scores_rating',\n",
    " 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin',\n",
    " 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'reviews_per_month']\n",
    "\n",
    "df.drop(review_cols_drop,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns that are based on review data\n",
    "\n",
    "host_cols_drop = ['host_url', 'host_name', 'host_location',\n",
    "                  'host_thumbnail_url', 'host_picture_url', 'host_neighbourhood',\n",
    "                  'host_listings_count', 'host_total_listings_count', 'calculated_host_listings_count_entire_homes',\n",
    "                  'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms',]\n",
    "\n",
    "df.drop(host_cols_drop,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns that can't be or would not be useful as predictor variables\n",
    "\n",
    "useless_cols_drop = ['scrape_id', 'last_scraped', 'picture_url', 'neighbourhood',\n",
    "                     'neighbourhood_group_cleansed', 'bathrooms', 'minimum_nights',\n",
    "                     'maximum_nights', 'minimum_minimum_nights', 'maximum_minimum_nights',\n",
    "                     'minimum_maximum_nights', 'maximum_maximum_nights', 'minimum_nights_avg_ntm',\n",
    "                     'maximum_nights_avg_ntm', 'calendar_updated', 'has_availability',\n",
    "                     'availability_30', 'availability_60', 'availability_90', 'availability_365',\n",
    "                     'calendar_last_scraped', 'license']\n",
    "\n",
    "df.drop(useless_cols_drop,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the variables with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>host_since</th>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_response_time</th>\n",
       "      <td>41905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_response_rate</th>\n",
       "      <td>41905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_acceptance_rate</th>\n",
       "      <td>39343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_is_superhost</th>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_has_profile_pic</th>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_identity_verified</th>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bathrooms_text</th>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bedrooms</th>\n",
       "      <td>4677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beds</th>\n",
       "      <td>1192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0\n",
       "host_since               1981\n",
       "host_response_time      41905\n",
       "host_response_rate      41905\n",
       "host_acceptance_rate    39343\n",
       "host_is_superhost        1981\n",
       "host_has_profile_pic     1981\n",
       "host_identity_verified   1981\n",
       "bathrooms_text            173\n",
       "bedrooms                 4677\n",
       "beds                     1192"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_values = pd.DataFrame(df.isnull().sum())\n",
    "null_values = null_values[null_values[0] != 0]\n",
    "null_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following variables all have blank values in their columns. I'm going to fill them in to avoid having to remove these properties from dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling the null values in the following columns rather than removing any rows\n",
    "\n",
    "df.name.fillna('null',inplace=True)\n",
    "df.description.fillna('null',inplace=True)\n",
    "df.neighborhood_overview.fillna('null',inplace=True)\n",
    "df.host_about.fillna('null',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've created a function to allow me to look at the distribution of values in each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for analysing a variable\n",
    "\n",
    "def variable_viewer(x):\n",
    "    values = df[x].value_counts(sort=False)\n",
    "    proportion = df[x].value_counts(sort=False,normalize='all')\n",
    "    variable_df = pd.DataFrame({'value_counts': values, 'proportion': proportion})\n",
    "    return variable_df.sort_values('value_counts', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new distance features using the long and lat variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance from \"centre\" of London"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making an assertion that trafalgar square is the centre of London."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafalgar_square = (51.504831314, -0.123499506)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['distance_from_center'] = df.apply(lambda row: geopy.distance.distance((row['latitude'],row['longitude']),trafalgar_square).km,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nearest train station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = pd.read_csv('../data/Stations_20180921.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to calculate the closest train station to each property and how far away it is in km."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def station_checker(lat,long):\n",
    "#     station = ''\n",
    "#     station_distance = 1000\n",
    "#     for station_,lat_, long_ in zip(stations.NAME,stations.y,stations.x):\n",
    "#         calculated_distance = geopy.distance.distance((lat,long),(lat_,long_)).km\n",
    "#         if calculated_distance < station_distance:\n",
    "#             station_distance = calculated_distance\n",
    "#             station = station_\n",
    "#     return station, station_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Christoph: this could be made quicker by using sklearn: Nearest Neighbours. \n",
    "\n",
    "Use the train stations\n",
    "\n",
    "Ehsan: pd.merge_asof(use co-ordinates as keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the results in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# station_dict = {'index': [], 'nearest_station': [], 'station_distance': []}\n",
    "\n",
    "\n",
    "# for i in df.index:\n",
    "#     station_checker_result = station_checker(df.loc[i]['latitude'],df.loc[i]['longitude'])\n",
    "#     station_dict['index'].append(i)\n",
    "#     station_dict['nearest_station'].append(station_checker_result[0])\n",
    "#     station_dict['station_distance'].append(station_checker_result[1])\n",
    "\n",
    "# station_df = pd.DataFrame(station_dict)\n",
    "# station_df.to_csv('../data/station_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe out of the station data\n",
    "\n",
    "station_df = pd.read_csv('../data/station_df.csv',index_col=1)\n",
    "\n",
    "station_df.drop('Unnamed: 0',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the series to our dataframe\n",
    "\n",
    "df['nearest_station'] = station_df.nearest_station\n",
    "df['station_distance'] = station_df.station_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding average rental price for the area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data sourced from https://www.ons.gov.uk/peoplepopulationandcommunity/housing/adhocs/12871privaterentalmarketinlondonjanuarytodecember2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "locator = geopy.geocoders.Nominatim(user_agent='myGeocoder',timeout=10)\n",
    "\n",
    "rgeocode = RateLimiter(locator.reverse, min_delay_seconds=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to find out the postcode of the property using the co-ordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def postcode_generator(row):\n",
    "#     co_ordinates = (row['y'],row['x'])\n",
    "#     try:\n",
    "#         location = rgeocode(co_ordinates)\n",
    "#         postcode = location.raw['address']['postcode'].split()[0]\n",
    "#         return postcode\n",
    "#     except:\n",
    "#         return \"error\"\n",
    "\n",
    "# stations['postcode'] = stations.apply(postcode_generator,axis=1)\n",
    "\n",
    "# stations.to_csv('../data/stations_with_postcode.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = pd.read_csv('../data/stations_with_postcode.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the stations dataframe for the join\n",
    "\n",
    "stations.rename(columns={'NAME': 'nearest_station'},inplace=True)\n",
    "\n",
    "stations.set_index('nearest_station',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining the stations data with our dataframe\n",
    "\n",
    "df = df.join(stations, on='nearest_station', how='left')\n",
    "\n",
    "# removing the columns we don't need\n",
    "\n",
    "df.drop(['FID','OBJECTID','EASTING','NORTHING','x','y', 'LINES'],axis=1,inplace=True)\n",
    "\n",
    "df.rename({'NETWORK':'rail_network','Zone':'tfl_zone'},axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the rental prices dataframe for the join\n",
    "\n",
    "rental_prices = pd.read_csv('../data/londonrentalstatisticsq42020.csv')\n",
    "rental_prices['Mean'] = rental_prices.Mean.apply(lambda x: float(x.replace(',','')))\n",
    "\n",
    "rental_prices.set_index('Postcode District',inplace=True)\n",
    "rental_prices.drop('Bedroom Category',axis=1,inplace=True)\n",
    "rental_prices.rename(columns={'Mean': 'mean_monthly_rent'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(rental_prices,on='postcode',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling the null values in the mean rent column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rent_filler(row):\n",
    "    if np.isnan(row['mean_monthly_rent']):\n",
    "        mean_neighbourhood_rent = df[df.neighbourhood_cleansed==row['neighbourhood_cleansed']]['mean_monthly_rent'].mean()\n",
    "        return mean_neighbourhood_rent\n",
    "    else:\n",
    "        return row['mean_monthly_rent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['mean_monthly_rent'] = df.apply(rent_filler,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing dollar sign from the price variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing dollar sign from price column and transforming values in to floats\n",
    "\n",
    "df['price'] = df.price.apply(lambda x: float(x.replace('$','').replace('.00','').replace(',','')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the properties with zero value for price from the dataframe. From looking at the Airbnb listings, these seem to be properties with zero availability. This is likely why Inside Airbnb were unable to scrape the data for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Removing the properties with a zero value for price\n",
    "\n",
    "# price_0 = df[df.price==0]\n",
    "\n",
    "# price_0.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(price_0.index,axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering the host response time column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'listing_url', 'name', 'description', 'neighborhood_overview',\n",
       "       'host_id', 'host_since', 'host_about', 'host_response_time',\n",
       "       'host_response_rate', 'host_acceptance_rate', 'host_is_superhost',\n",
       "       'host_verifications', 'host_has_profile_pic', 'host_identity_verified',\n",
       "       'neighbourhood_cleansed', 'latitude', 'longitude', 'property_type',\n",
       "       'room_type', 'accommodates', 'bathrooms_text', 'bedrooms', 'beds',\n",
       "       'amenities', 'price', 'instant_bookable',\n",
       "       'calculated_host_listings_count', 'distance_from_center',\n",
       "       'nearest_station', 'station_distance', 'rail_network', 'tfl_zone',\n",
       "       'postcode', 'mean_monthly_rent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN                   0.559928\n",
       "within an hour        0.224506\n",
       "within a few hours    0.093560\n",
       "within a day          0.076951\n",
       "a few days or more    0.045056\n",
       "Name: host_response_time, dtype: float64"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.host_response_time.value_counts(normalize='all',dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.host_response_time.fillna('unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unknown               0.559928\n",
       "within an hour        0.224506\n",
       "within a few hours    0.093560\n",
       "within a day          0.076951\n",
       "a few days or more    0.045056\n",
       "Name: host_response_time, dtype: float64"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.host_response_time.value_counts(normalize='all',dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['host_acceptance_rate'] = df.host_acceptance_rate.str.replace('%','').fillna(np.nan).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def host_filler(x):\n",
    "    if not np.isnan(x) and x >= 75:\n",
    "        return \"Above or equal to 75%\"\n",
    "    else:\n",
    "        return \"Below 75%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['host_acceptance_rate'] = df.host_acceptance_rate.apply(host_filler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['host_response_rate'] = df.host_response_rate.str.replace('%','').fillna(np.nan).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['host_response_rate'] = df.host_response_rate.apply(host_filler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the host_since, first_review and last_review columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the host since column to datetime\n",
    "\n",
    "df['host_since'] = pd.to_datetime(df.host_since)\n",
    "\n",
    "data_pulled_date = pd.to_datetime('2021-06-04')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes more sense to view how long a host has been active on Airbnb for then which date they joined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new column to show how many days the host has been active for\n",
    "\n",
    "def day_transformer(x):\n",
    "    delta = data_pulled_date - x\n",
    "    return delta.days\n",
    "\n",
    "df['host_since'] = df.host_since.apply(day_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll also use the previous function to transform the first_review and last_review columns\n",
    "\n",
    "df['first_review'] = pd.to_datetime(df['first_review'])\n",
    "df['last_review'] = pd.to_datetime(df['last_review'])\n",
    "\n",
    "df['first_review'] = df.first_review.apply(day_transformer)\n",
    "df['last_review'] = df.last_review.apply(day_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and filling the bathrooms_text variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the bathrooms_text variable from text to a continuous variable\n",
    "\n",
    "# function to check if string value is numeric\n",
    "\n",
    "def is_number(x):\n",
    "    try:\n",
    "        float(x)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# function to convert bathrooms_text values\n",
    "\n",
    "def bathroom_cleaner(x):\n",
    "    try:\n",
    "        split = x.lower().split()\n",
    "        if is_number(split[0]):\n",
    "            return float(split[0])\n",
    "        elif 'half-bath' in split:\n",
    "            return float(0.5)\n",
    "        else:\n",
    "            return float(x)\n",
    "    except:\n",
    "        return x\n",
    "    \n",
    "# replacing old bathrooms_text variable\n",
    "\n",
    "df.bathrooms_text = df.bathrooms_text.apply(bathroom_cleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling the bathrooms_text variable with the mean value depending on the room_type and bedrooms value of the property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bathrooms_notnull = df[(df.bathrooms_text.notna()) & (df.bedrooms.notna())][['bedrooms','room_type','bathrooms_text']]\n",
    "\n",
    "def bathroom_filler(row):\n",
    "    if np.isnan(row['bathrooms_text']):\n",
    "        try:\n",
    "            mean_value = round(bathrooms_notnull[(bathrooms_notnull.room_type==row['room_type']) & (bathrooms_notnull.bedrooms==row['bedrooms'])]['bathrooms_text'].mean())\n",
    "            return float(mean_value)\n",
    "        except:\n",
    "            return row['bathrooms_text'] \n",
    "    else:\n",
    "        return row['bathrooms_text']\n",
    "\n",
    "df['bathrooms_text'] = df.apply(bathroom_filler,axis=1)\n",
    "\n",
    "bathrooms_null = df[df.bathrooms_text.isna()]\n",
    "\n",
    "df.drop(bathrooms_null.index,axis=0,inplace=True)\n",
    "df['bathrooms_text'] = df.bathrooms_text.apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'bathrooms_text': 'bathrooms', 'neighbourhood_cleansed': 'neighbourhood'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling null values in beds and bedrooms columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding out the mean number of bedrooms for each room type\n",
    "\n",
    "bedrooms_notnull = df[(df.bedrooms.notnull())][['room_type','bedrooms','bathrooms']].copy()\n",
    "\n",
    "# code to replace null bedroom values with mean values based on room type and number of bathrooms\n",
    "\n",
    "\n",
    "def bedroom_cleaner(row):\n",
    "    if np.isnan(row['bedrooms']):\n",
    "        try:\n",
    "            mean_value = round(bedrooms_notnull[(bedrooms_notnull.room_type==row['room_type']) & (bedrooms_notnull.bathrooms==row['bathrooms'])]['bedrooms'].mean())\n",
    "            return mean_value\n",
    "        except:\n",
    "            return round(row['bathrooms'])\n",
    "    else:\n",
    "        return row['bedrooms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bedrooms'] = df.apply(bedroom_cleaner,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling the null and zero values in beds\n",
    "\n",
    "df['beds'] = df.apply(lambda row: row['bedrooms'] if np.isnan(row['beds']) or row['beds']==0 else row['beds'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the property_type column\n",
    "\n",
    "At the moment the property type column contains too many variables, some with very few values. I'm hoping that a model will perform better if these values are combined in to umbrella categories instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.property_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new property type categories\n",
    "\n",
    "property_categories = ['apartment', 'house', 'townhouse', 'condominium',\n",
    "                       'hotel', 'boutique hotel', 'bed and breakfast', 'loft',\n",
    "                       'guest suite', 'guesthouse', 'private room', 'aparthotel',\n",
    "                      'bungalow', 'hostel', 'boat', 'cottage', 'bungalow', 'villa', 'houseboat', 'other']\n",
    "\n",
    "# function to sort the property column in to new categories\n",
    "\n",
    "def property_cleaner(x):\n",
    "    split = x.lower().split()\n",
    "    if (' ').join(split[-3:]) in property_categories:\n",
    "        return (' ').join(split[-3:])\n",
    "    elif (' ').join(split[-2:]) in property_categories:\n",
    "        return (' ').join(split[-2:])\n",
    "    elif split[-1]=='houseboat':\n",
    "        return 'boat'\n",
    "    elif split[-1] in property_categories:\n",
    "        return split[-1]\n",
    "    else:\n",
    "        return 'other'\n",
    "    \n",
    "# apply function to property_type column\n",
    "\n",
    "df['property_type'] = df.property_type.apply(property_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new columns to show whether the properties have text variables such as description, host_about etc.\n",
    "\n",
    "df['description_provided'] = df.description.apply(lambda x: 0 if x == 'null' else 1)\n",
    "df['neighborhood_overview_provided'] = df.neighborhood_overview.apply(lambda x: 0 if x == 'null' else 1)\n",
    "df['host_about_provided'] = df.host_about.apply(lambda x: 0 if x == 'null' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling null values in reviews per month with 0\n",
    "\n",
    "df['reviews_per_month'].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values = pd.DataFrame(df.isnull().sum())\n",
    "null_values = null_values[null_values[0] != 0]\n",
    "null_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplifying the neighbourhood column\n",
    "\n",
    "There are a lot of values in the neighbourhood column. To at least make the EDA part of the process simpler, I'm going to divide the dataframe up in to TFL Zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zone_1 = ['Westminster', 'Tower Hamlets', 'Hackney', 'Kensington and Chelsea',\n",
    "#          'Camden', 'Islington', 'City of London']\n",
    "\n",
    "# zone_2 = ['Wandsworth', 'Hammersmith and Fulham', 'Greenwich',\n",
    "#          'Lewisham', 'Lambeth', 'Southwark']\n",
    "\n",
    "# zone_3 = ['Brent', 'Ealing', 'Croydon', 'Haringey',\n",
    "#          'Newham', 'Hounslow', 'Richmond upon Thames',\n",
    "#          'Barking and Dagenham', 'Merton']\n",
    "\n",
    "# zone_4 = ['Barnet', 'Bromley', 'Waltham Forest', 'Redbridge', 'Enfield',\n",
    "#          'Hillingdon', 'Harrow', 'Kingston upon Thames', 'Bexley', 'Sutton',\n",
    "#          'Havering']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def neighbourhood_cleaner(x):\n",
    "#     if x in zone_1:\n",
    "#         return \"Zone 1\"\n",
    "#     elif x in zone_2:\n",
    "#         return \"Zone 2\"\n",
    "#     elif x in zone_3:\n",
    "#         return \"Zone 3\"\n",
    "#     else:\n",
    "#         return \"Zone 4\"\n",
    "\n",
    "# df['tfl_zone'] = df.neighbourhood.apply(neighbourhood_cleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new variables to show length of text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_columns = ['name', 'description', 'neighborhood_overview', 'host_about']\n",
    "\n",
    "def text_counter(text):\n",
    "    if text != 'null':\n",
    "        split = text.split()\n",
    "        return len(split)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "for column in nlp_columns:\n",
    "    df[column+'_length'] = df[column].apply(text_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting true/false columns to binary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_binarise = ['host_is_superhost', 'host_has_profile_pic', 'host_identity_verified',\n",
    "                      'has_availability', 'instant_bookable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarise(x):\n",
    "    if x=='t':\n",
    "        return 1\n",
    "    elif x=='f':\n",
    "        return 0\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in columns_to_binarise:\n",
    "    df[column] = df[column].apply(binarise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing properties with no reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of the properties with insane outlier values have no reviews. Although this will remove a lot of observations from my data set, it will also provide me with more accurate data to build a model around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[(df.room_type=='Private room')&(df.price>750)].number_of_reviews.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[(df.room_type=='Entire home/apt')&(df.price>2000)].number_of_reviews.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing all properties that have never been reviewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df.number_of_reviews==0].index,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling in properties that have no host values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_sum = df.isnull().sum()\n",
    "\n",
    "null_sum[(null_sum!=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to fill the null values in the \"host\" columns with 0, under the assumption that they are new hosts and do not have the features represented by the other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling the host_since column with mean values\n",
    "\n",
    "df.host_since.fillna(value=df.host_since.mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_columns = ['host_is_superhost','host_listings_count','host_has_profile_pic','host_identity_verified']\n",
    "\n",
    "for column in host_columns:\n",
    "    df[column].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummifying the host_verification and amenities columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv_values = []\n",
    "\n",
    "for hv_list in df.host_verifications:\n",
    "    if eval(hv_list) != None:\n",
    "        lst = eval(hv_list)\n",
    "        for value in lst:\n",
    "            if value not in hv_values:\n",
    "                hv_values.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the list values in the host_verification column in to binary dummified columns\n",
    "\n",
    "# compiling all of the unique values within the lists\n",
    "\n",
    "hv_values = []\n",
    "\n",
    "for hv_list in df.host_verifications:\n",
    "    if eval(hv_list) != None:\n",
    "        lst = eval(hv_list)\n",
    "        for value in lst:\n",
    "            if value not in hv_values:\n",
    "                hv_values.append(value)\n",
    "            \n",
    "# creating a dictionary to store the binary values for each value    \n",
    "\n",
    "hv_dict = {}\n",
    "\n",
    "for value in hv_values:\n",
    "    hv_dict[value] = []\n",
    "\n",
    "# adding the binary values to the dictionary    \n",
    "    \n",
    "for hv_list in df.host_verifications:\n",
    "    if eval(hv_list) != None:\n",
    "        lst = eval(hv_list)\n",
    "        for key in hv_dict.keys():\n",
    "            if key in lst:\n",
    "                hv_dict[key].append(1)\n",
    "            else:\n",
    "                hv_dict[key].append(0)\n",
    "    else:\n",
    "        for key in hv_dict.keys():\n",
    "            hv_dict[key].append(0)\n",
    "            \n",
    "# checking that my dictionary has recorded a value for each observation\n",
    "\n",
    "for key in hv_dict.keys():\n",
    "    if len(hv_dict[key]) != df.shape[0]:\n",
    "        print(key, len(hv_dict[key]), \"error has occurred\")\n",
    "        \n",
    "# discarding the values that are present in a very small number of observations         \n",
    "        \n",
    "hv_columns = []\n",
    "\n",
    "for key in hv_dict.keys():\n",
    "    if sum(hv_dict[key]) >= df.shape[0]*0.01:\n",
    "        hv_columns.append(key)\n",
    "        \n",
    "# adding the columns to the dataframe\n",
    "\n",
    "for column in hv_columns:\n",
    "    df[\"host_verifications_\"+column] = hv_dict[column]\n",
    "    \n",
    "# dropping the host_verification column from my dataframe\n",
    "\n",
    "df.drop('host_verifications',axis=1, inplace=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the list values in the amenities column in to binary dummified columns\n",
    "\n",
    "amenities_values = []\n",
    "\n",
    "for amenities_list in df.amenities:\n",
    "    if eval(amenities_list) != None:\n",
    "        lst = eval(amenities_list)\n",
    "        for value in lst:\n",
    "            if value not in amenities_values:\n",
    "                amenities_values.append(value)\n",
    "            \n",
    "amenities_dict = {}\n",
    "\n",
    "for value in amenities_values:\n",
    "    amenities_dict[value] = []    \n",
    "    \n",
    "for amenities_list in df.amenities:\n",
    "    if eval(amenities_list) != None:\n",
    "        lst = eval(amenities_list)\n",
    "        for key in amenities_dict.keys():\n",
    "            if key in lst:\n",
    "                amenities_dict[key].append(1)\n",
    "            else:\n",
    "                amenities_dict[key].append(0)\n",
    "    else:\n",
    "        for key in amenities_dict.keys():\n",
    "            amenities_dict[key].append(0)\n",
    "            \n",
    "amenities_columns = []\n",
    "\n",
    "for key in amenities_dict.keys():\n",
    "    if sum(amenities_dict[key]) >= df.shape[0]*0.01:\n",
    "        amenities_columns.append(key)            \n",
    "        \n",
    "for column in amenities_columns:\n",
    "    df[\"amenities_\"+column] = amenities_dict[column]        \n",
    "    \n",
    "df.drop('amenities',axis=1, inplace=True)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing outliers\n",
    "\n",
    "#### Target Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of outliers in the target variable......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to make the assumption that a lot of these outliers are one of the following:\n",
    "\n",
    "- erroneously scraped (some of the properties have zero availability, which might have affected whichever software was used to scrape the data)\n",
    "- the price of the property has been raised by the host to prevent people from renting it (as an alternative to removing the listing?)\n",
    "- the property has been listed as a joke - see toilet room :)\n",
    "- the price has been set incorrectly by mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping outliers based on the conditions below\n",
    "\n",
    "room_outliers = df[(df.room_type=='Private room')&(df.price>1000)]\n",
    "house_outliers = df[(df.room_type=='Entire home/apt')&(df.price>10000)]\n",
    "\n",
    "df.drop(room_outliers.index, axis=0, inplace=True)\n",
    "df.drop(house_outliers.index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "sns.boxplot(x=df.price,y=df.room_type,ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the outliers in the Hotel room and Shared room categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.room_type=='Hotel room')&(df.price>500)].T.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These properties seem legitimate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.room_type=='Shared room')&(df.price>400)].T.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This property does not seem to be accurately priced. I'm going to remove it from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_rooms_drop = [37661065, 17420384, 21425945]\n",
    "\n",
    "df.drop(df[df.id.isin(shared_rooms_drop)].index,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the big price outliers in the Entire home/apt variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.room_type=='Entire home/apt')&(df.price>4000)].sort_values('price',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_houses_drop = [36657089, 11851238, 23706138, 39383869, 7974622, 40518546]\n",
    "\n",
    "\n",
    "df.drop(df[df.id.isin(entire_houses_drop)].index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "sns.boxplot(x=df.price,y=df.room_type,ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the outliers in the bathrooms and bedrooms categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This property gives a price per night for an individual property, yet lists all of the bathrooms and bedrooms for the range of properties the host offers on one one page: https://www.airbnb.com/rooms/43483035 65471\n",
    "https://www.airbnb.com/rooms/47089782 71819\n",
    "\n",
    "\n",
    "This property has erroneous listingsL: https://www.airbnb.com/rooms/40222389\t58992"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop_outliers = df[(df.listing_url=='https://www.airbnb.com/rooms/43483035')|(df.listing_url=='https://www.airbnb.com/rooms/47089782')|(df.listing_url=='https://www.airbnb.com/rooms/40222389')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing these properties from the dataset\n",
    "\n",
    "df.drop(df_drop_outliers.index,axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values = pd.DataFrame(df.isnull().sum())\n",
    "null_values = null_values[null_values[0] != 0]\n",
    "null_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are an insane amount of outliers in the target variable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.price.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.bedrooms.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('neighbourhood').count()['id'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "sns.boxplot(x=df.price,y=df.room_type,ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "sns.violinplot(x=df.price,y=df.room_type,ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "\n",
    "sns.heatmap(df[variables_continuous+['price']].corr(), mask=mask,square=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros_like(df[variables_continuous+['price']].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[np.triu_indices_from(mask)] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are there such expensive private rooms on Airbnb?\n",
    "\n",
    "Some of them must be priced as a joke!.....\n",
    "\n",
    "e.g. https://www.airbnb.com/rooms/10475894"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfl_zones = list(df.tfl_zone.unique())\n",
    "\n",
    "tfl_zones.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for zone in tfl_zones:\n",
    "    \n",
    "    df_zone = df[df.tfl_zone==zone]\n",
    "    \n",
    "    fig,ax = plt.subplots(figsize=(20,10))\n",
    "    sns.boxplot(x=df_zone.price,y=df_zone.room_type,ax=ax,)\n",
    "    ax.set_title(zone)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "df.groupby(['tfl_zone','room_type']).mean()[['price']].unstack().plot.bar(ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(10,6))\n",
    "\n",
    "df.groupby(['tfl_zone','room_type']).count().listing_url.unstack().plot.bar(ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "room_types = list(df.room_type.unique())\n",
    "\n",
    "for room in room_types:\n",
    "    fig,ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "    ax.hist(x=df[df.room_type==room].price, bins=40)\n",
    "    ax.set_title(room)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeoPandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df = gpd.read_file('../data/neighbourhoods.geojson')\n",
    "map_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the empty column\n",
    "map_df.drop('neighbourhood_group', axis=1, inplace=True)\n",
    "\n",
    "# Creating a dataframe of listing counts and median price by borough\n",
    "neighbourhood_df = pd.DataFrame(df.groupby('neighbourhood').size())\n",
    "neighbourhood_df.rename(columns={0: 'number_of_listings'}, inplace=True)\n",
    "neighbourhood_df['mean_price'] = df.groupby('neighbourhood').price.mean().values\n",
    "\n",
    "# Joining the dataframes\n",
    "neighbourhood_map_df = map_df.set_index('neighbourhood').join(neighbourhood_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting the number of listings in each borough\n",
    "fig1, ax1 = plt.subplots(1, figsize=(15, 6))\n",
    "neighbourhood_map_df.plot(column='number_of_listings', cmap='Reds', ax=ax1)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Number of Airbnb listings in each London neighbourhood', fontsize=14)\n",
    "sm = plt.cm.ScalarMappable(cmap='Reds', norm=plt.Normalize(vmin=0, vmax=9000))\n",
    "sm._A = [] # Creates an empty array for the data range\n",
    "cbar = fig1.colorbar(sm)\n",
    "plt.show()\n",
    "\n",
    "# Plotting the mean price of listings in each borough\n",
    "fig2, ax2 = plt.subplots(1, figsize=(15, 6))\n",
    "neighbourhood_map_df.plot(column='mean_price', cmap='Greens', ax=ax2)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Mean price of Airbnb listings in each London neighbourhood', fontsize=14)\n",
    "sm = plt.cm.ScalarMappable(cmap='Greens', norm=plt.Normalize(vmin=min(neighbourhood_map_df.mean_price), vmax=max(neighbourhood_map_df.mean_price)))\n",
    "sm._A = [] # Creates an empty array for the data range\n",
    "cbar = fig2.colorbar(sm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "credit for the code above to https://github.com/L-Lewis/Airbnb-neural-network-price-prediction/blob/master/Airbnb-price-prediction.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_corr = pd.DataFrame(df.corr()['price'])\n",
    "\n",
    "df_corr['av_correlation'] = df_corr.price.apply(lambda x: abs(x))\n",
    "\n",
    "df_corr.sort_values('av_correlation', ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairplot = df[['beds', 'mean_monthly_rent', 'accommodates',\n",
    "       'bathrooms', 'bedrooms','price', 'tfl_zone', 'room_type']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(pairplot,hue='room_type')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['tfl_zone', 'property_type', 'room_type', 'accommodates',\n",
    "       'bathrooms', 'bedrooms','distance_from_center','station_distance',\n",
    "       'rail_network', 'mean_monthly_rent', 'postcode']].copy()\n",
    "\n",
    "y = df.price.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up train and test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "categorical_variables = ['tfl_zone', 'property_type', 'room_type',\n",
    "                        'rail_network', 'postcode']\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "one_hot = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('dummy', one_hot, categorical_variables)],\n",
    "remainder='passthrough',\n",
    "sparse_threshold=0)\n",
    "\n",
    "model= ElasticNetCV(max_iter=10000)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Elastic CV Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Best Alpha: {}\".format(model.alpha_))\n",
    "print(\"Best l1_ratio: {}\".format(model.l1_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the parameters obtained from the elastic CV search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "categorical_variables = ['tfl_zone', 'property_type', 'room_type',\n",
    "                        'rail_network', 'postcode']\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "one_hot = OneHotEncoder(sparse=False,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('dummy', one_hot, categorical_variables)],\n",
    "remainder='passthrough',\n",
    "sparse_threshold=0)\n",
    "\n",
    "model= ElasticNet(alpha=0.3547,l1_ratio=0.5, max_iter=10000)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.scatterplot(y=pipe.predict(X_train), x=y_train, color='b', ax=ax)\n",
    "ax.plot([df.price.min(), df.price.max()], [\n",
    "        df.price.min(), df.price.max()], lw=2, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = PolynomialFeatures(degree=2, include_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                         [('pf'), pf],\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.scatterplot(y=pipe.predict(X_train), x=y_train,hue=X_train.tfl_zone, color='b', ax=ax)\n",
    "ax.plot([df.price.min(), df.price.max()], [\n",
    "        df.price.min(), df.price.max()], lw=2, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.scatterplot(y=pipe.predict(X_train), x=y_train,hue=X_train.room_type, color='b', ax=ax)\n",
    "ax.plot([df.price.min(), df.price.max()], [\n",
    "        df.price.min(), df.price.max()], lw=2, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.scatterplot(y=pipe.predict(X_test), x=y_test,hue=X_test.tfl_zone, color='b', ax=ax)\n",
    "ax.plot([df.price.min(), df.price.max()], [\n",
    "        df.price.min(), df.price.max()], lw=2, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.scatterplot(y=pipe.predict(X_test), x=y_test,hue=X_test.room_type, color='b', ax=ax)\n",
    "ax.plot([df.price.min(), df.price.max()], [\n",
    "        df.price.min(), df.price.max()], lw=2, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeRegressor(max_depth=5)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('decision_tree', decision_tree)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestRegressor(\n",
    "                           n_estimators=100,max_depth=40,n_jobs=-2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('random_forest', random_forest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All non-NLP features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.neighborhood_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_corr = pd.DataFrame(df.corr()['price'])\n",
    "\n",
    "df_corr['av_correlation'] = df_corr.price.apply(lambda x: abs(x))\n",
    "\n",
    "df_corr.sort_values('av_correlation', ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_discard = ['id','listing_url', 'latitude', 'longitude', 'has_availability', 'availability_60',\n",
    "                     'availability_30', 'availability_90', 'availability_365', 'number_of_reviews',\n",
    "                     'number_of_reviews_ltm', 'number_of_reviews_l30d', 'first_review', 'last_review',\n",
    "                     'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness',\n",
    "                      'review_scores_checkin', 'review_scores_communication', 'review_scores_location',\n",
    "                     'review_scores_value','calculated_host_listings_count','calculated_host_listings_count_entire_homes',\n",
    "                     'calculated_host_listings_count_private_rooms','calculated_host_listings_count_shared_rooms',\n",
    "                     'reviews_per_month', 'nearest_station'\n",
    "                     ]\n",
    "\n",
    "variables_nlp = ['name', 'description', 'neighborhood_overview', 'host_about']\n",
    "\n",
    "variables_continuous = ['host_since', 'host_listings_count', 'accommodates', 'bathrooms',\n",
    "                       'bedrooms', 'beds', 'distance_from_center', 'station_distance',\n",
    "                       'mean_monthly_rent', 'name_length', 'description_length', 'neighborhood_overview_length',\n",
    "                       'host_about_length']\n",
    "\n",
    "variables_dummify = ['neighbourhood', 'property_type', 'room_type', 'rail_network',\n",
    "                    'tfl_zone', 'postcode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(variables_continuous+variables_discard+variables_dummify+variables_nlp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "X.drop(variables_discard+variables_nlp, axis=1, inplace=True)\n",
    "\n",
    "y = X.pop('price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "one_hot = OneHotEncoder(sparse=False, handle_unknown='ignore',)\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough',\n",
    "sparse_threshold=0)\n",
    "\n",
    "model= ElasticNetCV(max_iter=10000)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up train and test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Elastic CV Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Best Alpha: {}\".format(model.alpha_))\n",
    "print(\"Best l1_ratio: {}\".format(model.l1_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "one_hot = OneHotEncoder(sparse=False, handle_unknown='ignore',)\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough',\n",
    "sparse_threshold=0)\n",
    "\n",
    "model= ElasticNet(alpha=model.alpha_,max_iter=10000)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeRegressor(max_depth=5)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('decision_tree', decision_tree)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestRegressor(\n",
    "                           n_estimators=100,max_depth=50,n_jobs=-2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('random_forest', random_forest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.scatterplot(y=pipe.predict(X_train), x=y_train,hue=X_train.tfl_zone, color='b', ax=ax)\n",
    "ax.plot([df.price.min(), df.price.max()], [\n",
    "        df.price.min(), df.price.max()], lw=2, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.scatterplot(y=pipe.predict(X_train), x=y_train,hue=X_train.room_type, color='b', ax=ax)\n",
    "ax.plot([df.price.min(), df.price.max()], [\n",
    "        df.price.min(), df.price.max()], lw=2, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.scatterplot(y=pipe.predict(X_test), x=y_test,hue=X_test.tfl_zone, color='b', ax=ax)\n",
    "ax.plot([df.price.min(), df.price.max()], [\n",
    "        df.price.min(), df.price.max()], lw=2, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.scatterplot(y=pipe.predict(X_test), x=y_test,hue=X_test.room_type, color='b', ax=ax)\n",
    "ax.plot([df.price.min(), df.price.max()], [\n",
    "        df.price.min(), df.price.max()], lw=2, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including NLP\n",
    "\n",
    "\n",
    "### Count Vectorizer\n",
    "\n",
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stop_words = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "custom_stop_words = []\n",
    "\n",
    "for word in english_stop_words:\n",
    "    custom_stop_words.append(word)\n",
    "custom_stop_words.append('null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in nlp_columns:\n",
    "    X[column] = df[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up train and test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.95,max_features=4000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "model= Lasso(max_iter=10000, random_state=1)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, ngram_range=(1,3), min_df=10, max_df=0.95, max_features = 4000)\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "model= Lasso(max_iter=10000, random_state=1)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "tvec = TfidfVectorizer(stop_words=custom_stop_words, ngram_range=(1,1), min_df=10, max_df=0.95)\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('tvec_name', tvec, 'name'),\n",
    " ('tvec_description', tvec, 'description'),\n",
    " ('tvec_neighbourhood_overview', tvec, 'neighborhood_overview'),\n",
    " ('tvec_host_about', tvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "model= Lasso(max_iter=10000, random_state=1)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5,n_jobs=-2)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "tvec = TfidfVectorizer(stop_words=custom_stop_words, ngram_range=(1,1), min_df=10, max_df=0.95, max_features = 4000)\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('tvec_name', tvec, 'name'),\n",
    " ('tvec_description', tvec, 'description'),\n",
    " ('tvec_neighbourhood_overview', tvec, 'neighborhood_overview'),\n",
    " ('tvec_host_about', tvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "model= Lasso(max_iter=10000, random_state=1)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5,n_jobs=-2)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "tvec = TfidfVectorizer(stop_words=custom_stop_words, ngram_range=(1,2), min_df=10, max_df=0.95, max_features = 4000)\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('tvec_name', tvec, 'name'),\n",
    " ('tvec_description', tvec, 'description'),\n",
    " ('tvec_neighbourhood_overview', tvec, 'neighborhood_overview'),\n",
    " ('tvec_host_about', tvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "model= Lasso(max_iter=5000, random_state=1)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5,n_jobs=-2)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Attempt 1 - higher max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.95,max_features=8000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "model= Lasso(max_iter=10000, random_state=1)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1 - Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.95,max_features=4000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "decision_tree = DecisionTreeRegressor(max_depth=10)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('decision_tree', decision_tree)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.95,max_features=8000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "decision_tree = DecisionTreeRegressor(max_depth=10)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('decision_tree', decision_tree)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1 - Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.95,max_features=4000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "random_forest = RandomForestRegressor(\n",
    "                           n_estimators=100,max_depth=30,n_jobs=-2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('random_forest', random_forest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.95,max_features=4000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "random_forest = RandomForestRegressor(\n",
    "                           n_estimators=100,max_depth=30,n_jobs=-2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('random_forest', random_forest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-IDF - Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "tvec = TfidfVectorizer(stop_words=custom_stop_words, ngram_range=(1,1), min_df=10, max_df=0.95, max_features = 4000)\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('tvec_name', tvec, 'name'),\n",
    " ('tvec_description', tvec, 'description'),\n",
    " ('tvec_neighbourhood_overview', tvec, 'neighborhood_overview'),\n",
    " ('tvec_host_about', tvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "random_forest = RandomForestRegressor(\n",
    "                           n_estimators=100,max_depth=60,n_jobs=-2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5,n_jobs=-2)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.95,max_features=3000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "random_forest = RandomForestRegressor(\n",
    "                           n_estimators=100,max_depth=30,n_jobs=-2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('random_forest', random_forest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.95,max_features=4000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "random_forest = RandomForestRegressor(\n",
    "                           n_estimators=100,max_depth=50,n_jobs=-2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('random_forest', random_forest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.95,max_features=2000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "random_forest = RandomForestRegressor(\n",
    "                           n_estimators=100,max_depth=30,n_jobs=-2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('random_forest', random_forest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=50, max_df=0.95,max_features=3000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "random_forest = RandomForestRegressor(\n",
    "                           n_estimators=100,max_depth=30,n_jobs=-2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('random_forest', random_forest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=100, max_df=0.95,max_features=3000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "random_forest = RandomForestRegressor(\n",
    "                           n_estimators=100,max_depth=30,n_jobs=-2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('random_forest', random_forest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.90,max_features=3000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "random_forest = RandomForestRegressor(\n",
    "                           n_estimators=100,max_depth=30,n_jobs=-2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('random_forest', random_forest)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Score so far!\n",
    "\n",
    "Grid search the model to get a better score (higher max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.90,max_features=3000,ngram_range=(1, 2))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "random_forest = RandomForestRegressor(\n",
    "                           n_estimators=100,max_depth=30,n_jobs=-2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('random_forest', random_forest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.90,max_features=3000,ngram_range=(2, 2))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "random_forest = RandomForestRegressor(\n",
    "                           n_estimators=100,max_depth=30,n_jobs=-2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('random_forest', random_forest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.90,max_features=3000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "random_forest = RandomForestRegressor()\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('random_forest', random_forest)],verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'random_forest__n_estimators': [120],\n",
    "         'random_forest__max_depth': [30, 60],\n",
    "         'random_forest__min_samples_leaf': [1, 10],\n",
    "         'random_forest__max_samples': [None, 0.8],\n",
    "         'random_forest__max_features': [None, 0.8]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_random_forest = GridSearchCV(pipe, params, cv=5, n_jobs=-2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_random_forest.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training Score: {}\".format(gs_random_forest.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(gs_random_forest.score(X_test,y_test)))\n",
    "print(\"CV Mean Score: {}\".format(gs_random_forest.best_score_))\n",
    "print(\"Best Model Parameters: {}\".format(gs_random_forest.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_random_forest.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(pipe, 'pipe_capstone_random_forest.jlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(gs_random_forest.best_estimator_, 'grid_search_capstone_random_forest.jlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_random_forest = joblib.load('grid_search_capstone_random_forest.jlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_random_forest.named_steps['random_forest']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm Start Estimator Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.90,max_features=3000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('gs_random_forest', gs_random_forest.named_steps['random_forest'])],verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'gs_random_forest__n_estimators': [100, 150, 200],\n",
    "         'gs_random_forest__warm_start': [True]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_random_forest_estimators = GridSearchCV(pipe, params, cv=5, n_jobs=-2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_random_forest_estimators.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training Score: {}\".format(gs_random_forest_estimators.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(gs_random_forest_estimators.score(X_test,y_test)))\n",
    "print(\"CV Mean Score: {}\".format(gs_random_forest_estimators.best_score_))\n",
    "print(\"Best Model Parameters: {}\".format(gs_random_forest_estimators.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(pipe, 'pipe_capstone_random_forest_estimator.jlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(gs_random_forest_estimators.best_estimator_, 'grid_search_capstone_random_forest_estimator.jlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.95,max_features=4000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "model= LassoCV(max_iter=10000, random_state=1)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "\n",
    "print(\"Elastic CV Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Best Alpha: {}\".format(model.alpha_))\n",
    "print(\"Best l1_ratio: {}\".format(model.l1_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.95,max_features=4000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "model= RidgeCV(max_iter=10000, random_state=1)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "\n",
    "print(\"Elastic CV Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Best Alpha: {}\".format(model.alpha_))\n",
    "print(\"Best l1_ratio: {}\".format(model.l1_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-running best-performing random forest model for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.90,max_features=3000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "model = joblib.load('grid_search_capstone_random_forest_estimator.jlib').named_steps['gs_random_forest']\n",
    "model.set_params(warm_start=False, n_jobs=-2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)],verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5, n_jobs=-3)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to get feature names out of the pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pipe.named_steps['col_trans'].get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_dummify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(pipe.named_steps['model'].feature_importances_,\n",
    "             columns=['importance'],\n",
    "             index=pipe.named_steps['col_trans'].get_feature_names()\n",
    "             ).sort_values(by='importance', ascending=False\n",
    "                           ).iloc[:50].plot(kind='barh', figsize=(8, 14))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[df.name.str.contains('-55%')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2,figsize=(20, 8),sharey=True)\n",
    "\n",
    "sns.scatterplot(y=pipe.predict(X_test), x=y_test,hue=X_test.room_type, ax=ax[0])\n",
    "ax[0].plot([df.price.min(), df.price.max()], [\n",
    "        df.price.min(), df.price.max()], lw=2, c='r')\n",
    "\n",
    "ax[0].set(xlabel=\"True Price\", ylabel = \"Predicted Price\")\n",
    "\n",
    "sns.scatterplot(y=pipe.predict(X_test), x=y_test,hue=X_test.tfl_zone, color='g', palette='dark', ax=ax[1])\n",
    "ax[1].plot([df.price.min(), df.price.max()], [\n",
    "        df.price.min(), df.price.max()], lw=2, c='r')\n",
    "\n",
    "ax[1].set(xlabel=\"True Price\", ylabel = \"Predicted Price\")\n",
    "\n",
    "fig.suptitle('Comparison of predicted results in the test set with true values', fontsize=15)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_predictions = X_test.copy()\n",
    "\n",
    "X_test_predictions['true_price'] = y_test\n",
    "X_test_predictions['predicted_price'] = pipe.predict(X_test)\n",
    "X_test_predictions['residual_values'] = pipe.predict(X_test) - y_test\n",
    "X_test_predictions['abs_residual_values'] = abs(pipe.predict(X_test) - y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[68934]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_test_predictions[['accommodates','neighbourhood','tfl_zone', 'name', 'description','host_about','true_price','predicted_price',\n",
    "                   'residual_values', 'abs_residual_values']].sort_values('abs_residual_values',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.price.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amenities_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.name=='MAYFAIR HOUSE - DELUXE & MODERN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.name=='Cosy home in seven sisters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.name=='-55% Vibrant Studio Near Holborn Tube Station']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting the number of listings in each borough\n",
    "fig1, ax1 = plt.subplots(1, figsize=(15, 6))\n",
    "neighbourhood_map_df.plot(column='number_of_listings', cmap='Reds', ax=ax1)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Number of Airbnb listings in each London neighbourhood', fontsize=14)\n",
    "sm = plt.cm.ScalarMappable(cmap='Reds', norm=plt.Normalize(vmin=0, vmax=9000))\n",
    "sm._A = [] # Creates an empty array for the data range\n",
    "cbar = fig1.colorbar(sm)\n",
    "plt.show()\n",
    "\n",
    "# Plotting the mean price of listings in each borough\n",
    "fig2, ax2 = plt.subplots(1, figsize=(15, 6))\n",
    "neighbourhood_map_df.plot(column='mean_price', cmap='Greens', ax=ax2)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Mean price of Airbnb listings in each London neighbourhood', fontsize=14)\n",
    "sm = plt.cm.ScalarMappable(cmap='Greens', norm=plt.Normalize(vmin=min(neighbourhood_map_df.mean_price), vmax=max(neighbourhood_map_df.mean_price)))\n",
    "sm._A = [] # Creates an empty array for the data range\n",
    "cbar = fig2.colorbar(sm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_long_lat = X_test_predictions.join(df[['longitude','latitude']],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_long_lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_predictions.abs_residual_values.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "cmap = sns.cubehelix_palette(as_cmap=True)\n",
    "\n",
    "sns.scatterplot(x='latitude',y='longitude',data=test_long_lat, hue='abs_residual_values',palette=cmap)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2,figsize=(20, 8),sharey=True)\n",
    "\n",
    "sns.scatterplot(y=pipe.predict(X_train), x=y_train,hue=X_train.room_type, ax=ax[0])\n",
    "ax[0].plot([df.price.min(), df.price.max()], [\n",
    "        df.price.min(), df.price.max()], lw=2, c='r')\n",
    "\n",
    "ax[0].set(xlabel=\"True Price\", ylabel = \"Predicted Price\")\n",
    "\n",
    "sns.scatterplot(y=pipe.predict(X_train), x=y_train,hue=X_train.tfl_zone, color='g', palette='dark', ax=ax[1])\n",
    "ax[1].plot([df.price.min(), df.price.max()], [\n",
    "        df.price.min(), df.price.max()], lw=2, c='r')\n",
    "\n",
    "ax[1].set(xlabel=\"True Price\", ylabel = \"Predicted Price\")\n",
    "\n",
    "fig.suptitle('Comparison of predicted results in the training set with true values', fontsize=15)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = pipe.predict(X_test)-y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.hist(residuals, bins=50)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Residuals Plot',size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.90,max_features=3000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "model = RandomForestRegressor(n_estimators: 150, max_depth=60, max_features=0.8, n_jobs=0.2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)],verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only looking at features (no review information)\n",
    "\n",
    "This model will only include very basic predictor variables, to get an idea of how well this dataset performs at predicting property prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # instantiating a new dataframe to only look at features\n",
    "\n",
    "# df_features = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # columns to be removed\n",
    "\n",
    "# columns_to_drop = null_values.index\n",
    "# columns_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # removing the review features from my dataframe\n",
    "\n",
    "# df_features.drop(columns_to_drop,axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df_features.copy()\n",
    "# X.drop(['longitude', 'latitude'],axis=1,inplace=True)\n",
    "\n",
    "# y = X.pop('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.head().T.iloc[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # columns to one-hot encode\n",
    "\n",
    "# one_hot_columns = ['host_is_superhost', 'host_has_profile_pic', 'host_identity_verified', 'neighbourhood_cleansed',\n",
    "#                    'property_type', 'room_type', 'has_availability', 'instant_bookable']\n",
    "\n",
    "# # columns to countvectorize for NLP\n",
    "\n",
    "# nlp_columns = ['name', 'description', 'neighborhood_overview', 'host_about']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Plan\n",
    "\n",
    "Perform modelling on features without reviews first? Then model including reviews.\n",
    "\n",
    "Capture metadata aspects about the reviews? HOw many reviews and over which timeframe?\n",
    "\n",
    "- Create data dictionary - DONE\n",
    "- Data Cleaning - DONE\n",
    "- EDA - partial\n",
    "- Feature Engineering + Further Data Cleaning - partial\n",
    "- Linear Regression or Classification? - DONE\n",
    "- Fit Model on Listings Dataset to Predict Prices - DONE\n",
    "- Fit Model on Reviews Dataset to Predict Prices - DONE\n",
    "- Combine Both to Predict Prices - DONE\n",
    "- Visualise findings - use the Tableau location function\n",
    "- Perform Clustering on the Reviews - what insights can we gather? Create word clouds\n",
    "- Predict reviews based on NLP of reviews\n",
    "- What are people looking for when they stay at an Airbnb?\n",
    "- Which neighborhoods are the most popular? Which are the most expensive?\n",
    "- Can we see any trends on where people like to stay?\n",
    "- Are there other features that we can use from different datasets\n",
    "\n",
    "When transforming data - do train and test split before transforming. This means that your model isn't already aware words that appear in your test set. You need to turn-off drop first, though, and set the parameter to ignore any unknown words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- can we apply the model to other cities?\n",
    "\n",
    "To-Do List\n",
    "\n",
    "Data Cleaning:\n",
    "\n",
    "- use median values rather than mean values (mean values will be swayed more by outliers)\n",
    "- simplify the categorisation of the property type variable\n",
    "- apply lower and higher limits to the price variable to deal with outliers\n",
    "- simplify the amenities + host binarised variables\n",
    "- create a new column to show the average property price for each host_id\n",
    "- bring in geographical proximity of attractions as target variables\n",
    "\n",
    "Variable Transformation:\n",
    "\n",
    "- look at distributions of continuous/discrete variables - do they need transforming?\n",
    "- look in to log transforming the continuous variables (naive-Bayes lessons)\n",
    "\n",
    "Modelling:\n",
    "\n",
    "- review the use of NLP - could we instead look at key words within the variables? This might be a better option for the title of the \n",
    "- can we use neural networks?\n",
    "\n",
    "good visualisations: https://towardsdatascience.com/predicting-airbnb-prices-with-deep-learning-part-2-how-to-improve-your-nightly-price-50ea8bc2bd29"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
