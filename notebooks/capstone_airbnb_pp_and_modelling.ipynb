{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries here\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV, Lasso, LassoCV, Ridge, RidgeCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the visualisation dataframe\n",
    "\n",
    "df_vis = joblib.load('../data/jlib_files/dataframes/complete_df_2_modelling.jlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the adapted dataframe for modelling\n",
    "\n",
    "df = joblib.load('../data/jlib_files/dataframes/modelling_df_2_modelling.jlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without text variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge, Lasso and Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "\n",
    "y = X.pop('price')\n",
    "\n",
    "X.drop(['name', 'description', 'neighborhood_overview', 'host_about'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up train and test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['host_response_time', 'host_response_rate', 'host_acceptance_rate',\n",
       "       'host_is_superhost', 'host_has_profile_pic', 'host_identity_verified',\n",
       "       'neighbourhood', 'room_type', 'accommodates', 'bathrooms', 'bedrooms',\n",
       "       'beds', 'instant_bookable', 'calculated_host_listings_count',\n",
       "       'property_type_basic', 'air_conditioning', 'bbq', 'baby_facilities',\n",
       "       'balcony_or_patio', 'bath', 'bed_linen', 'cable_tv', 'child_friendly',\n",
       "       'coffee_maker', 'cooking_facilities', 'dishwasher', 'garden',\n",
       "       'has_workspace', 'host_greets_you', 'long_term_stays',\n",
       "       'luggage_dropoff', 'lock_on_bedroom_door', 'luxury_facilities',\n",
       "       'private_entrance', 'toiletries', 'tumble_dryer', 'tv',\n",
       "       'distance_from_center', 'station_distance', 'mean_monthly_rent',\n",
       "       'serviced_property', 'description_provided', 'host_about_provided',\n",
       "       'name_length', 'description_length', 'neighborhood_overview_length',\n",
       "       'host_about_length'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "categorical_variables = ['host_response_time', 'host_response_rate', 'host_acceptance_rate',\n",
    "                         'neighbourhood', 'room_type', 'property_type_basic']\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "one_hot = OneHotEncoder(sparse=False, drop='first')\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('dummy', one_hot, categorical_variables)],\n",
    "remainder='passthrough',\n",
    "sparse_threshold=0)\n",
    "\n",
    "model= ElasticNetCV(alphas=np.logspace(-4, 4, 10), \n",
    "                     l1_ratio=np.array([0.00001, .1, .5, .7, .9, .95, .99, 1]),\n",
    "                     cv=5, max_iter=100000)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic CV Score: 0.4078694938065287\n",
      "Best Alpha: 0.046415888336127774\n",
      "Best l1_ratio: 0.99\n"
     ]
    }
   ],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "# cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Elastic CV Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Best Alpha: {}\".format(model.alpha_))\n",
    "print(\"Best l1_ratio: {}\".format(model.l1_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RidgeCV Score: 0.4079238913860872\n",
      "Best Alpha: 166.81005372000558\n"
     ]
    }
   ],
   "source": [
    "model = RidgeCV(alphas=np.logspace(-4, 4, 10),cv=5)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])\n",
    "\n",
    "pipe.fit(X_train,y_train)\n",
    "# cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"RidgeCV Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Best Alpha: {}\".format(model.alpha_))\n",
    "# print(\"Best l1_ratio: {}\".format(model.l1_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic CV Score: 0.4078708872518245\n",
      "Best Alpha: 0.046415888336127774\n"
     ]
    }
   ],
   "source": [
    "model = LassoCV(alphas=np.logspace(-4, 4, 10),cv=5)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])\n",
    "\n",
    "pipe.fit(X_train,y_train)\n",
    "# cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Elastic CV Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Best Alpha: {}\".format(model.alpha_))\n",
    "# print(\"Best l1_ratio: {}\".format(model.l1_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the parameters obtained from the elastic CV search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "categorical_variables = ['tfl_zone', 'property_type', 'room_type',\n",
    "                        'rail_network', 'postcode']\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "one_hot = OneHotEncoder(sparse=False,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('dummy', one_hot, categorical_variables)],\n",
    "remainder='passthrough',\n",
    "sparse_threshold=0)\n",
    "\n",
    "model= ElasticNet(alpha=0.3547,l1_ratio=0.5, max_iter=10000)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.scatterplot(y=pipe.predict(X_train), x=y_train, color='b', ax=ax)\n",
    "ax.plot([df.price.min(), df.price.max()], [\n",
    "        df.price.min(), df.price.max()], lw=2, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = PolynomialFeatures(degree=2, include_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                         [('pf'), pf],\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesradford/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 16158595.741263151, tolerance: 76885.88537150917\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/jamesradford/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 124150006.91475841, tolerance: 76885.88537150917\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/jamesradford/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 151348908.91068608, tolerance: 76885.88537150917\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/jamesradford/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 162326660.6075146, tolerance: 76885.88537150917\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/jamesradford/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 19449515.174726605, tolerance: 76567.43142207994\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/jamesradford/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 125313247.75990373, tolerance: 76567.43142207994\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/jamesradford/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 152794780.64515287, tolerance: 76567.43142207994\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/jamesradford/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 161306579.20770666, tolerance: 76567.43142207994\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/jamesradford/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 21941204.007069618, tolerance: 75229.53069933348\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/jamesradford/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 110047260.01968113, tolerance: 75229.53069933348\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/jamesradford/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 138286452.32708335, tolerance: 75229.53069933348\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/jamesradford/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 159152044.50654888, tolerance: 75229.53069933348\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/jamesradford/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18655320.533365786, tolerance: 76796.1301400218\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/jamesradford/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 119589862.56660444, tolerance: 76796.1301400218\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/jamesradford/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 146218532.8097578, tolerance: 76796.1301400218\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/jamesradford/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 159888509.33319336, tolerance: 76796.1301400218\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/jamesradford/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 23292858.470441222, tolerance: 76174.04400169538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/jamesradford/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 126443601.64339839, tolerance: 76174.04400169538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/jamesradford/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 152836210.82444975, tolerance: 76174.04400169538\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-40bf4287b0d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcv_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Score: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test Score: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1299\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mthis_l1_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis_alphas\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1_ratios\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m                 for train, test in folds)\n\u001b[0;32m-> 1301\u001b[0;31m         mse_paths = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[1;32m   1302\u001b[0m                              **_joblib_parallel_args(prefer=\"threads\"))(jobs)\n\u001b[1;32m   1303\u001b[0m         \u001b[0mmse_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_l1_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    864\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py\u001b[0m in \u001b[0;36m_path_residuals\u001b[0;34m(X, y, train, test, path, path_params, alphas, l1_ratio, X_order, dtype)\u001b[0m\n\u001b[1;32m   1109\u001b[0m     X_train = check_array(X_train, accept_sparse='csc', dtype=dtype,\n\u001b[1;32m   1110\u001b[0m                           order=X_order)\n\u001b[0;32m-> 1111\u001b[0;31m     \u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpath_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py\u001b[0m in \u001b[0;36mlasso_path\u001b[0;34m(X, y, eps, n_alphas, alphas, precompute, Xy, copy_X, coef_init, verbose, return_n_iter, positive, **params)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_encode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \"\"\"\n\u001b[0;32m--> 311\u001b[0;31m     return enet_path(X, y, l1_ratio=1., eps=eps, n_alphas=n_alphas,\n\u001b[0m\u001b[1;32m    312\u001b[0m                      \u001b[0malphas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecompute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecompute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mXy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                      \u001b[0mcopy_X\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoef_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py\u001b[0m in \u001b[0;36menet_path\u001b[0;34m(X, y, l1_ratio, eps, n_alphas, alphas, precompute, Xy, copy_X, coef_init, verbose, return_n_iter, positive, check_input, **params)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 precompute = check_array(precompute, dtype=X.dtype.type,\n\u001b[1;32m    524\u001b[0m                                          order='C')\n\u001b[0;32m--> 525\u001b[0;31m             model = cd_fast.enet_coordinate_descent_gram(\n\u001b[0m\u001b[1;32m    526\u001b[0m                 \u001b[0mcoef_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecompute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m                 tol, rng, random, positive)\n",
      "\u001b[0;32msklearn/linear_model/_cd_fast.pyx\u001b[0m in \u001b[0;36msklearn.linear_model._cd_fast.enet_coordinate_descent_gram\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.scatterplot(y=pipe.predict(X_train), x=y_train,hue=X_train.tfl_zone, color='b', ax=ax)\n",
    "ax.plot([df.price.min(), df.price.max()], [\n",
    "        df.price.min(), df.price.max()], lw=2, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.scatterplot(y=pipe.predict(X_train), x=y_train,hue=X_train.room_type, color='b', ax=ax)\n",
    "ax.plot([df.price.min(), df.price.max()], [\n",
    "        df.price.min(), df.price.max()], lw=2, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.scatterplot(y=pipe.predict(X_test), x=y_test,hue=X_test.tfl_zone, color='b', ax=ax)\n",
    "ax.plot([df.price.min(), df.price.max()], [\n",
    "        df.price.min(), df.price.max()], lw=2, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.scatterplot(y=pipe.predict(X_test), x=y_test,hue=X_test.room_type, color='b', ax=ax)\n",
    "ax.plot([df.price.min(), df.price.max()], [\n",
    "        df.price.min(), df.price.max()], lw=2, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeRegressor(max_depth=5)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('decision_tree', decision_tree)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestRegressor(\n",
    "                           n_estimators=100,max_depth=40,n_jobs=-2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('random_forest', random_forest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All non-NLP features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.neighborhood_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_corr = pd.DataFrame(df.corr()['price'])\n",
    "\n",
    "df_corr['av_correlation'] = df_corr.price.apply(lambda x: abs(x))\n",
    "\n",
    "df_corr.sort_values('av_correlation', ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_discard = ['id','listing_url', 'latitude', 'longitude', 'has_availability', 'availability_60',\n",
    "                     'availability_30', 'availability_90', 'availability_365', 'number_of_reviews',\n",
    "                     'number_of_reviews_ltm', 'number_of_reviews_l30d', 'first_review', 'last_review',\n",
    "                     'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness',\n",
    "                      'review_scores_checkin', 'review_scores_communication', 'review_scores_location',\n",
    "                     'review_scores_value','calculated_host_listings_count','calculated_host_listings_count_entire_homes',\n",
    "                     'calculated_host_listings_count_private_rooms','calculated_host_listings_count_shared_rooms',\n",
    "                     'reviews_per_month', 'nearest_station'\n",
    "                     ]\n",
    "\n",
    "variables_nlp = ['name', 'description', 'neighborhood_overview', 'host_about']\n",
    "\n",
    "variables_continuous = ['host_since', 'host_listings_count', 'accommodates', 'bathrooms',\n",
    "                       'bedrooms', 'beds', 'distance_from_center', 'station_distance',\n",
    "                       'mean_monthly_rent', 'name_length', 'description_length', 'neighborhood_overview_length',\n",
    "                       'host_about_length']\n",
    "\n",
    "variables_dummify = ['neighbourhood', 'property_type', 'room_type', 'rail_network',\n",
    "                    'tfl_zone', 'postcode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(variables_continuous+variables_discard+variables_dummify+variables_nlp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "X.drop(variables_discard+variables_nlp, axis=1, inplace=True)\n",
    "\n",
    "y = X.pop('price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "one_hot = OneHotEncoder(sparse=False, handle_unknown='ignore',)\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough',\n",
    "sparse_threshold=0)\n",
    "\n",
    "model= ElasticNetCV(max_iter=10000)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up train and test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Elastic CV Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Best Alpha: {}\".format(model.alpha_))\n",
    "print(\"Best l1_ratio: {}\".format(model.l1_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "one_hot = OneHotEncoder(sparse=False, handle_unknown='ignore',)\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough',\n",
    "sparse_threshold=0)\n",
    "\n",
    "model= ElasticNet(alpha=model.alpha_,max_iter=10000)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeRegressor(max_depth=5)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('decision_tree', decision_tree)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestRegressor(\n",
    "                           n_estimators=100,max_depth=50,n_jobs=-2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('random_forest', random_forest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.scatterplot(y=pipe.predict(X_train), x=y_train,hue=X_train.tfl_zone, color='b', ax=ax)\n",
    "ax.plot([df.price.min(), df.price.max()], [\n",
    "        df.price.min(), df.price.max()], lw=2, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.scatterplot(y=pipe.predict(X_train), x=y_train,hue=X_train.room_type, color='b', ax=ax)\n",
    "ax.plot([df.price.min(), df.price.max()], [\n",
    "        df.price.min(), df.price.max()], lw=2, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.scatterplot(y=pipe.predict(X_test), x=y_test,hue=X_test.tfl_zone, color='b', ax=ax)\n",
    "ax.plot([df.price.min(), df.price.max()], [\n",
    "        df.price.min(), df.price.max()], lw=2, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.scatterplot(y=pipe.predict(X_test), x=y_test,hue=X_test.room_type, color='b', ax=ax)\n",
    "ax.plot([df.price.min(), df.price.max()], [\n",
    "        df.price.min(), df.price.max()], lw=2, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including NLP\n",
    "\n",
    "\n",
    "### Count Vectorizer\n",
    "\n",
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stop_words = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "custom_stop_words = []\n",
    "\n",
    "for word in english_stop_words:\n",
    "    custom_stop_words.append(word)\n",
    "custom_stop_words.append('null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in nlp_columns:\n",
    "    X[column] = df[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up train and test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.95,max_features=4000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "model= Lasso(max_iter=10000, random_state=1)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, ngram_range=(1,3), min_df=10, max_df=0.95, max_features = 4000)\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "model= Lasso(max_iter=10000, random_state=1)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "tvec = TfidfVectorizer(stop_words=custom_stop_words, ngram_range=(1,1), min_df=10, max_df=0.95)\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('tvec_name', tvec, 'name'),\n",
    " ('tvec_description', tvec, 'description'),\n",
    " ('tvec_neighbourhood_overview', tvec, 'neighborhood_overview'),\n",
    " ('tvec_host_about', tvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "model= Lasso(max_iter=10000, random_state=1)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5,n_jobs=-2)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "tvec = TfidfVectorizer(stop_words=custom_stop_words, ngram_range=(1,1), min_df=10, max_df=0.95, max_features = 4000)\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('tvec_name', tvec, 'name'),\n",
    " ('tvec_description', tvec, 'description'),\n",
    " ('tvec_neighbourhood_overview', tvec, 'neighborhood_overview'),\n",
    " ('tvec_host_about', tvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "model= Lasso(max_iter=10000, random_state=1)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5,n_jobs=-2)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "tvec = TfidfVectorizer(stop_words=custom_stop_words, ngram_range=(1,2), min_df=10, max_df=0.95, max_features = 4000)\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('tvec_name', tvec, 'name'),\n",
    " ('tvec_description', tvec, 'description'),\n",
    " ('tvec_neighbourhood_overview', tvec, 'neighborhood_overview'),\n",
    " ('tvec_host_about', tvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "model= Lasso(max_iter=5000, random_state=1)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5,n_jobs=-2)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Attempt 1 - higher max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.95,max_features=8000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "model= Lasso(max_iter=10000, random_state=1)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1 - Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.95,max_features=4000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "decision_tree = DecisionTreeRegressor(max_depth=10)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('decision_tree', decision_tree)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.95,max_features=8000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "decision_tree = DecisionTreeRegressor(max_depth=10)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('decision_tree', decision_tree)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1 - Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.95,max_features=4000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "random_forest = RandomForestRegressor(\n",
    "                           n_estimators=100,max_depth=30,n_jobs=-2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('random_forest', random_forest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.95,max_features=4000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "random_forest = RandomForestRegressor(\n",
    "                           n_estimators=100,max_depth=30,n_jobs=-2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('random_forest', random_forest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-IDF - Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "tvec = TfidfVectorizer(stop_words=custom_stop_words, ngram_range=(1,1), min_df=10, max_df=0.95, max_features = 4000)\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('tvec_name', tvec, 'name'),\n",
    " ('tvec_description', tvec, 'description'),\n",
    " ('tvec_neighbourhood_overview', tvec, 'neighborhood_overview'),\n",
    " ('tvec_host_about', tvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "random_forest = RandomForestRegressor(\n",
    "                           n_estimators=100,max_depth=60,n_jobs=-2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5,n_jobs=-2)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.95,max_features=3000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "random_forest = RandomForestRegressor(\n",
    "                           n_estimators=100,max_depth=30,n_jobs=-2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('random_forest', random_forest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.95,max_features=4000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "random_forest = RandomForestRegressor(\n",
    "                           n_estimators=100,max_depth=50,n_jobs=-2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('random_forest', random_forest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.95,max_features=2000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "random_forest = RandomForestRegressor(\n",
    "                           n_estimators=100,max_depth=30,n_jobs=-2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('random_forest', random_forest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=50, max_df=0.95,max_features=3000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "random_forest = RandomForestRegressor(\n",
    "                           n_estimators=100,max_depth=30,n_jobs=-2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('random_forest', random_forest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=100, max_df=0.95,max_features=3000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "random_forest = RandomForestRegressor(\n",
    "                           n_estimators=100,max_depth=30,n_jobs=-2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('random_forest', random_forest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.90,max_features=3000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "random_forest = RandomForestRegressor(\n",
    "                           n_estimators=100,max_depth=30,n_jobs=-2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('random_forest', random_forest)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Score so far!\n",
    "\n",
    "Grid search the model to get a better score (higher max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.90,max_features=3000,ngram_range=(1, 2))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "random_forest = RandomForestRegressor(\n",
    "                           n_estimators=100,max_depth=30,n_jobs=-2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('random_forest', random_forest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.90,max_features=3000,ngram_range=(2, 2))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "random_forest = RandomForestRegressor(\n",
    "                           n_estimators=100,max_depth=30,n_jobs=-2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('random_forest', random_forest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.90,max_features=3000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "random_forest = RandomForestRegressor()\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('random_forest', random_forest)],verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'random_forest__n_estimators': [120],\n",
    "         'random_forest__max_depth': [30, 60],\n",
    "         'random_forest__min_samples_leaf': [1, 10],\n",
    "         'random_forest__max_samples': [None, 0.8],\n",
    "         'random_forest__max_features': [None, 0.8]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_random_forest = GridSearchCV(pipe, params, cv=5, n_jobs=-2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_random_forest.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training Score: {}\".format(gs_random_forest.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(gs_random_forest.score(X_test,y_test)))\n",
    "print(\"CV Mean Score: {}\".format(gs_random_forest.best_score_))\n",
    "print(\"Best Model Parameters: {}\".format(gs_random_forest.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_random_forest.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(pipe, 'pipe_capstone_random_forest.jlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(gs_random_forest.best_estimator_, 'grid_search_capstone_random_forest.jlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_random_forest = joblib.load('grid_search_capstone_random_forest.jlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_random_forest.named_steps['random_forest']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm Start Estimator Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.90,max_features=3000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('gs_random_forest', gs_random_forest.named_steps['random_forest'])],verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'gs_random_forest__n_estimators': [100, 150, 200],\n",
    "         'gs_random_forest__warm_start': [True]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_random_forest_estimators = GridSearchCV(pipe, params, cv=5, n_jobs=-2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_random_forest_estimators.fit(X_train,y_train)\n",
    "\n",
    "print(\"Training Score: {}\".format(gs_random_forest_estimators.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(gs_random_forest_estimators.score(X_test,y_test)))\n",
    "print(\"CV Mean Score: {}\".format(gs_random_forest_estimators.best_score_))\n",
    "print(\"Best Model Parameters: {}\".format(gs_random_forest_estimators.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(pipe, 'pipe_capstone_random_forest_estimator.jlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(gs_random_forest_estimators.best_estimator_, 'grid_search_capstone_random_forest_estimator.jlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.95,max_features=4000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "model= LassoCV(max_iter=10000, random_state=1)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "\n",
    "print(\"Elastic CV Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Best Alpha: {}\".format(model.alpha_))\n",
    "print(\"Best l1_ratio: {}\".format(model.l1_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.95,max_features=4000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "model= RidgeCV(max_iter=10000, random_state=1)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "\n",
    "print(\"Elastic CV Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Best Alpha: {}\".format(model.alpha_))\n",
    "print(\"Best l1_ratio: {}\".format(model.l1_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-running best-performing random forest model for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.90,max_features=3000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "model = joblib.load('grid_search_capstone_random_forest_estimator.jlib').named_steps['gs_random_forest']\n",
    "model.set_params(warm_start=False, n_jobs=-2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)],verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5, n_jobs=-3)\n",
    "\n",
    "print(\"Training Score: {}\".format(pipe.score(X_train,y_train)))\n",
    "print(\"Test Score: {}\".format(pipe.score(X_test,y_test)))\n",
    "print(\"CV Scores: {}\".format(cv_scores))\n",
    "print(\"CV Mean Score: {}\".format(cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to get feature names out of the pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pipe.named_steps['col_trans'].get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_dummify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(pipe.named_steps['model'].feature_importances_,\n",
    "             columns=['importance'],\n",
    "             index=pipe.named_steps['col_trans'].get_feature_names()\n",
    "             ).sort_values(by='importance', ascending=False\n",
    "                           ).iloc[:50].plot(kind='barh', figsize=(8, 14))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[df.name.str.contains('-55%')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2,figsize=(20, 8),sharey=True)\n",
    "\n",
    "sns.scatterplot(y=pipe.predict(X_test), x=y_test,hue=X_test.room_type, ax=ax[0])\n",
    "ax[0].plot([df.price.min(), df.price.max()], [\n",
    "        df.price.min(), df.price.max()], lw=2, c='r')\n",
    "\n",
    "ax[0].set(xlabel=\"True Price\", ylabel = \"Predicted Price\")\n",
    "\n",
    "sns.scatterplot(y=pipe.predict(X_test), x=y_test,hue=X_test.tfl_zone, color='g', palette='dark', ax=ax[1])\n",
    "ax[1].plot([df.price.min(), df.price.max()], [\n",
    "        df.price.min(), df.price.max()], lw=2, c='r')\n",
    "\n",
    "ax[1].set(xlabel=\"True Price\", ylabel = \"Predicted Price\")\n",
    "\n",
    "fig.suptitle('Comparison of predicted results in the test set with true values', fontsize=15)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_predictions = X_test.copy()\n",
    "\n",
    "X_test_predictions['true_price'] = y_test\n",
    "X_test_predictions['predicted_price'] = pipe.predict(X_test)\n",
    "X_test_predictions['residual_values'] = pipe.predict(X_test) - y_test\n",
    "X_test_predictions['abs_residual_values'] = abs(pipe.predict(X_test) - y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[68934]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_test_predictions[['accommodates','neighbourhood','tfl_zone', 'name', 'description','host_about','true_price','predicted_price',\n",
    "                   'residual_values', 'abs_residual_values']].sort_values('abs_residual_values',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.price.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amenities_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.name=='MAYFAIR HOUSE - DELUXE & MODERN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.name=='Cosy home in seven sisters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.name=='-55% Vibrant Studio Near Holborn Tube Station']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting the number of listings in each borough\n",
    "fig1, ax1 = plt.subplots(1, figsize=(15, 6))\n",
    "neighbourhood_map_df.plot(column='number_of_listings', cmap='Reds', ax=ax1)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Number of Airbnb listings in each London neighbourhood', fontsize=14)\n",
    "sm = plt.cm.ScalarMappable(cmap='Reds', norm=plt.Normalize(vmin=0, vmax=9000))\n",
    "sm._A = [] # Creates an empty array for the data range\n",
    "cbar = fig1.colorbar(sm)\n",
    "plt.show()\n",
    "\n",
    "# Plotting the mean price of listings in each borough\n",
    "fig2, ax2 = plt.subplots(1, figsize=(15, 6))\n",
    "neighbourhood_map_df.plot(column='mean_price', cmap='Greens', ax=ax2)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Mean price of Airbnb listings in each London neighbourhood', fontsize=14)\n",
    "sm = plt.cm.ScalarMappable(cmap='Greens', norm=plt.Normalize(vmin=min(neighbourhood_map_df.mean_price), vmax=max(neighbourhood_map_df.mean_price)))\n",
    "sm._A = [] # Creates an empty array for the data range\n",
    "cbar = fig2.colorbar(sm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_long_lat = X_test_predictions.join(df[['longitude','latitude']],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_long_lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_predictions.abs_residual_values.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "cmap = sns.cubehelix_palette(as_cmap=True)\n",
    "\n",
    "sns.scatterplot(x='latitude',y='longitude',data=test_long_lat, hue='abs_residual_values',palette=cmap)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2,figsize=(20, 8),sharey=True)\n",
    "\n",
    "sns.scatterplot(y=pipe.predict(X_train), x=y_train,hue=X_train.room_type, ax=ax[0])\n",
    "ax[0].plot([df.price.min(), df.price.max()], [\n",
    "        df.price.min(), df.price.max()], lw=2, c='r')\n",
    "\n",
    "ax[0].set(xlabel=\"True Price\", ylabel = \"Predicted Price\")\n",
    "\n",
    "sns.scatterplot(y=pipe.predict(X_train), x=y_train,hue=X_train.tfl_zone, color='g', palette='dark', ax=ax[1])\n",
    "ax[1].plot([df.price.min(), df.price.max()], [\n",
    "        df.price.min(), df.price.max()], lw=2, c='r')\n",
    "\n",
    "ax[1].set(xlabel=\"True Price\", ylabel = \"Predicted Price\")\n",
    "\n",
    "fig.suptitle('Comparison of predicted results in the training set with true values', fontsize=15)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = pipe.predict(X_test)-y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.hist(residuals, bins=50)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Residuals Plot',size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the pipeline to transform the data\n",
    "\n",
    "# instantiating my transformers\n",
    "\n",
    "cvec = CountVectorizer(stop_words=custom_stop_words, min_df=10, max_df=0.90,max_features=3000,ngram_range=(1, 1))\n",
    "one_hot = OneHotEncoder(sparse=True,handle_unknown='ignore')\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "\n",
    "\n",
    "col_trans = ColumnTransformer(\n",
    "[('cvec_name', cvec, 'name'),\n",
    " ('cvec_description', cvec, 'description'),\n",
    " ('cvec_neighbourhood_overview', cvec, 'neighborhood_overview'),\n",
    " ('cvec_host_about', cvec, 'host_about'),\n",
    " ('dummy', one_hot, variables_dummify)],\n",
    "remainder='passthrough')\n",
    "\n",
    "model = RandomForestRegressor(n_estimators: 150, max_depth=60, max_features=0.8, n_jobs=0.2)\n",
    "\n",
    "pipe = Pipeline(steps = [('col_trans', col_trans),\n",
    "                        ('scaler', scaler),\n",
    "                        ('model', model)],verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only looking at features (no review information)\n",
    "\n",
    "This model will only include very basic predictor variables, to get an idea of how well this dataset performs at predicting property prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # instantiating a new dataframe to only look at features\n",
    "\n",
    "# df_features = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # columns to be removed\n",
    "\n",
    "# columns_to_drop = null_values.index\n",
    "# columns_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # removing the review features from my dataframe\n",
    "\n",
    "# df_features.drop(columns_to_drop,axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df_features.copy()\n",
    "# X.drop(['longitude', 'latitude'],axis=1,inplace=True)\n",
    "\n",
    "# y = X.pop('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.head().T.iloc[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # columns to one-hot encode\n",
    "\n",
    "# one_hot_columns = ['host_is_superhost', 'host_has_profile_pic', 'host_identity_verified', 'neighbourhood_cleansed',\n",
    "#                    'property_type', 'room_type', 'has_availability', 'instant_bookable']\n",
    "\n",
    "# # columns to countvectorize for NLP\n",
    "\n",
    "# nlp_columns = ['name', 'description', 'neighborhood_overview', 'host_about']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Plan\n",
    "\n",
    "Perform modelling on features without reviews first? Then model including reviews.\n",
    "\n",
    "Capture metadata aspects about the reviews? HOw many reviews and over which timeframe?\n",
    "\n",
    "- Create data dictionary - DONE\n",
    "- Data Cleaning - DONE\n",
    "- EDA - partial\n",
    "- Feature Engineering + Further Data Cleaning - partial\n",
    "- Linear Regression or Classification? - DONE\n",
    "- Fit Model on Listings Dataset to Predict Prices - DONE\n",
    "- Fit Model on Reviews Dataset to Predict Prices - DONE\n",
    "- Combine Both to Predict Prices - DONE\n",
    "- Visualise findings - use the Tableau location function\n",
    "- Perform Clustering on the Reviews - what insights can we gather? Create word clouds\n",
    "- Predict reviews based on NLP of reviews\n",
    "- What are people looking for when they stay at an Airbnb?\n",
    "- Which neighborhoods are the most popular? Which are the most expensive?\n",
    "- Can we see any trends on where people like to stay?\n",
    "- Are there other features that we can use from different datasets\n",
    "\n",
    "When transforming data - do train and test split before transforming. This means that your model isn't already aware words that appear in your test set. You need to turn-off drop first, though, and set the parameter to ignore any unknown words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- can we apply the model to other cities?\n",
    "\n",
    "To-Do List\n",
    "\n",
    "Data Cleaning:\n",
    "\n",
    "- use median values rather than mean values (mean values will be swayed more by outliers)\n",
    "- simplify the categorisation of the property type variable\n",
    "- apply lower and higher limits to the price variable to deal with outliers\n",
    "- simplify the amenities + host binarised variables\n",
    "- create a new column to show the average property price for each host_id\n",
    "- bring in geographical proximity of attractions as target variables\n",
    "\n",
    "Variable Transformation:\n",
    "\n",
    "- look at distributions of continuous/discrete variables - do they need transforming?\n",
    "- look in to log transforming the continuous variables (naive-Bayes lessons)\n",
    "\n",
    "Modelling:\n",
    "\n",
    "- review the use of NLP - could we instead look at key words within the variables? This might be a better option for the title of the \n",
    "- can we use neural networks?\n",
    "\n",
    "good visualisations: https://towardsdatascience.com/predicting-airbnb-prices-with-deep-learning-part-2-how-to-improve-your-nightly-price-50ea8bc2bd29"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
